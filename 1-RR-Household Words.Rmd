---
title: "1-RR-Household Words"
author: "Anoff Nicholas Cobblah"
date: "July 30, 2018"
output: html_document
    number_sections: yes
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### March 2018: "play", "player", "recreation", "work", "worker", and "labor" in *Household Words*

My current contention in my dissertation is that work rhetoric rises in the BAAS publications because science is being more associated with work. But to really test this, it would be helpful to have some nonscientific publications to compare to. Maybe work rhetoric is just becoming more prevalent across Victorian discourse? This is the first step in checking this. What I find is not that revolutionary: work was referneced more than play, but in this 10 year run there isn't a lot of change, actually.

This script combines my Word Flagging and KWIC (tokenizer script) methods in order to create an interactive illustration of the frequency with which the terms "play", "player", "recreation", "work", "worker", and "labor" were referenced in *Household Words*. The goal is to determine the context for work and play rhetoric in that publication.

```{r Household Words Parameters,  eval=FALSE}
    HouseholdWordslocation <- paste0(getwd())
    HouseholdWordsdoclocation <- paste0(HouseholdWordslocation,"/Documents")
    HouseholdWordslongconlength <- 250
    HouseholdWordsshortconlength <- 3
    HouseholdWordsPOSconlength <- 10
    HouseholdWordsplaysearchedtermlist <- c("play", "player", "recreation")
    HouseholdWordsworksearchedtermlist <- c("work", "worker","labor")
    HouseholdWordssearchedtermlist <- c(HouseholdWordsplaysearchedtermlist,HouseholdWordsworksearchedtermlist)
    HouseholdWordsoutputlocation <- HouseholdWordslocation
    HouseholdWordsWordFlagdfPath <- paste0(HouseholdWordsoutputlocation,"/","HouseholdWordsWordFlagdf.txt")
    HouseholdWordsDocumentSize <- 92623162
```

**IMPORTANT NOTE: Since creating a Word Flag matrix can take a nontrivial amount of time for larger corpuses, this script is designed only to run the program to create a new HouseholdWordsWordFlagdf if there is a change to the dataset in folder "Documents" or if the previous HouseholdWordsWordFlagdf has been deleted.**

To create the data frame compiling every reference to a term, run the following script.

```{r MarHouseholdWordsApp Word Flag,  eval=FALSE}
      if(sum(file.info(list.files(HouseholdWordsdoclocation, all.files = TRUE, recursive = TRUE, full.names=TRUE))$size) == HouseholdWordsDocumentSize) {
        HouseholdWordsDataChange1 <- FALSE
        print("The data in the 'Documents' folder appears not to have changed.")
      }else{
        HouseholdWordsDataChange1 <- TRUE
        print("The data in the 'Documents' folder appears to have been changed. A new HouseholdWordsWordFlagdf will therefore be created. TO UPDATE THIS SCRIPT, PLEASE CHANGE THE HouseholdWordsDocumentSize TO REFLECT THE NEW SIZE OF THE DOCUMENTS.")
        }
      
      if(file.exists(HouseholdWordsWordFlagdfPath) == TRUE) {
        HouseholdWordsDataChange2 <- FALSE
        print("The previous HouseholdWordsWordFlagdf still exists.")
      }else{
        HouseholdWordsDataChange2 <- TRUE
        print("The previous HouseholdWordswordFlagdf seems to have been moved or deleted.  A new HouseholdWordsWordFlag will therefore be created.")
        }

  if(HouseholdWordsDataChange1|HouseholdWordsDataChange2 == TRUE) {
  
      files <- list.files(path = HouseholdWordsdoclocation, pattern = "txt", full.names = TRUE) #creates vector of txt file names.
      if(file.exists(HouseholdWordsoutputlocation) == FALSE){dir.create(HouseholdWordsoutputlocation)}
      HouseholdWordsstemsearchedtermlist <- unique(wordStem(HouseholdWordssearchedtermlist)) #lemmatizes the list of terms you want to search for.
      HouseholdWordsWordFlagmat <- matrix(,ncol=12,nrow=1)
      for (i in 1:length(files)) {
        fileName <- read_file(files[i])
        Encoding(fileName) <- "UTF-8"  #since tokenize_sentences function requires things to be encoded in UTF-8, need to remove some data.
        fileName <- iconv(fileName, "UTF-8", "UTF-8",sub='')
        ltoken <- tokenize_words(fileName, lowercase = TRUE, stopwords = NULL, simplify = FALSE)
        ltoken <- unlist(ltoken)
        stemltoken <- wordStem(ltoken) #this uses the Snowball library to lemmatize the entire text.
        textID <- i
        for (p in 1:length(HouseholdWordsstemsearchedtermlist)) {
          HouseholdWordsstemsearchedterm <- HouseholdWordsstemsearchedtermlist[p]
          for (j in 1:length(stemltoken)) {
              if (HouseholdWordsstemsearchedterm == stemltoken[j]) {
                if (j <= HouseholdWordslongconlength) {longtempvec <- ltoken[(1:(j+HouseholdWordslongconlength))]}
                if (j > HouseholdWordslongconlength) {longtempvec <- ltoken[(j-HouseholdWordslongconlength):(j+HouseholdWordslongconlength)]}
                if (j <= HouseholdWordsshortconlength) {shorttempvec <- ltoken[(1:(j+HouseholdWordsshortconlength))]}
                if (j > HouseholdWordsshortconlength) {shorttempvec <- ltoken[(j-HouseholdWordsshortconlength):(j+HouseholdWordsshortconlength)]}
                if (j <= HouseholdWordsPOSconlength) {POStempvec <- ltoken[(1:(j+HouseholdWordsPOSconlength))]}
                if (j > HouseholdWordsPOSconlength) {POStempvec <- ltoken[(j-HouseholdWordsPOSconlength):(j+HouseholdWordsPOSconlength)]}
                TempTextName <- gsub(paste0(HouseholdWordsdoclocation,"/"),"",files[i]) #This grabs just the end of the file path.
                TempTextName <- gsub(".txt","",TempTextName) #This removes the .txt from the end of the name.
                temprow <- matrix(,ncol=12,nrow=1)
                colnames(temprow) <- c("Text", "Text_ID", "HouseholdWordsstemsearchedterm","Lemma","Lemma_Perc","KWIC","Total_Lemma","Date","Category","Short_KWIC","POS_KWIC","Current_Date")
                temprow[1,1] <- TempTextName
                temprow[1,2] <- textID
                temprow[1,3] <- HouseholdWordsstemsearchedterm
                temprow[1,4] <- j
                temprow[1,5] <- (j/length(stemltoken))*100
                temprow[1,6] <- as.character(paste(longtempvec,sep= " ",collapse=" "))
                temprow[1,7] <- length(stemltoken)
                temprow[1,8] <- strsplit(TempTextName,"_")[[1]][2]
                #Determining Category
                  if(HouseholdWordsstemsearchedterm %in% wordStem(HouseholdWordsplaysearchedtermlist)) {temprow[1,9] <- "Play-Rhetoric"}
                  if(HouseholdWordsstemsearchedterm %in% wordStem(HouseholdWordsworksearchedtermlist)) {temprow[1,9] <- "Work-Rhetoric"}
                temprow[1,10] <- as.character(paste(shorttempvec,sep= " ",collapse=" "))
                temprow[1,11] <- as.character(paste(POStempvec,sep= " ",collapse=" "))
                temprow[1,12] <- format(Sys.time(), "%Y-%m-%d")
                HouseholdWordsWordFlagmat <- rbind(HouseholdWordsWordFlagmat,temprow)
              }
          }
        }
        print(paste0(i," out of ",length(files))) #let's user watch as code runs for long searches
      }
      HouseholdWordsWordFlagmat <- HouseholdWordsWordFlagmat[-1,]
      HouseholdWordsWordFlagdf <- as.data.frame(HouseholdWordsWordFlagmat)
      write.table(HouseholdWordsWordFlagdf, HouseholdWordsWordFlagdfPath)
      HouseholdWordsWordFlagdf[1:5,]
  }else{
    print("Loading the previous dataset as HouseholdWordsWordFlagdf")
    HouseholdWordsWordFlagdf <- read.table(HouseholdWordsWordFlagdfPath)
  }
HouseholdWordsWordFlagdf
```

We can then add up the values in HouseholdWordsWordFlagdf to make a table of the frequency of play and work rhetoric, HouseholdWordsFreqmat. It's important to do it this way because it allows us to choose a random KWIC.

```{r HouseholdWordsFreqmat,  eval=FALSE}
  # Adding values from HouseholdWordsWordFlagdf together to get a matrix of normalized frequencies for each category, as HouseholdWordsFreqmat
  HouseholdWordsWordFlagPlaydf <- HouseholdWordsWordFlagdf[grep("Play-Rhetoric",HouseholdWordsWordFlagdf$Category),]
      HouseholdWordsWordFlagWorkdf <- HouseholdWordsWordFlagdf[grep("Work-Rhetoric",HouseholdWordsWordFlagdf$Category),]
      HouseholdWordsFreqmat <- matrix(,ncol=9,nrow=1)
      files <- list.files(path = HouseholdWordsdoclocation, pattern = "txt", full.names = TRUE)
      for (i in 1:length(files)) {
        TempTextName <- gsub(paste0(HouseholdWordsdoclocation,"/"),"",files[i]) #This grabs just the end of the file path.
        TempTextName <- gsub(".txt","",TempTextName) #This removes the .txt from the end of the name.
        tempplaydf <- HouseholdWordsWordFlagPlaydf[grep(TempTextName,HouseholdWordsWordFlagPlaydf$Text),]
        tempworkdf <- HouseholdWordsWordFlagWorkdf[grep(TempTextName,HouseholdWordsWordFlagWorkdf$Text),]
        TempDate <- strsplit(TempTextName,"_")[[1]][2]
        TempLength <- tempplaydf$Total_Lemma[1]
        temprows <- matrix(,ncol=9,nrow=2)
        colnames(temprows) <- c("Text", "Text_ID","Date","Category","Frequency","Total_Lemma","Normalized_Freq","Sample_KWIC","Avg_Lemma_Perc")
        temprows[1:2,1] <- as.character(TempTextName)
        temprows[1:2,2] <- i
        temprows[1:2,3] <- as.character(TempDate)
        temprows[1,4] <- "Play-Rhetoric"
        temprows[2,4] <- "Work-Rhetoric"
        temprows[1,5] <- nrow(tempplaydf)
        temprows[2,5] <- nrow(tempworkdf)
        temprows[1:2,6]<- as.character(TempLength)
        temprows[1,7] <- (as.numeric(temprows[1,5])/as.numeric(temprows[1,6]))*100
        temprows[2,7] <- (as.numeric(temprows[2,5])/as.numeric(temprows[2,6]))*100
        #temprows[1,8]
          if(nrow(tempplaydf) > 0){temprows[1,8] <- as.character(sample(tempplaydf$Short_KWIC,1))}else{temprows[1,8] <- NA}
        #temprows[2,8]
          if(nrow(tempworkdf) >0) {temprows[2,8] <- as.character(sample(tempworkdf$Short_KWIC,1))}else{temprows[2,8] <- NA}
        temprows[1,9] <- mean(as.numeric(as.character(tempplaydf$Lemma_Perc)))
        temprows[2,9] <- mean(as.numeric(as.character(tempworkdf$Lemma_Perc)))
        HouseholdWordsFreqmat <- rbind(HouseholdWordsFreqmat,temprows)
      }
      HouseholdWordsFreqmat <- HouseholdWordsFreqmat[-1,]
      HouseholdWordsFreqdf <- as.data.frame(HouseholdWordsFreqmat)
      HouseholdWordsFreqdf
```

With the data in hand, we can now ask some questions about our corpus, such as: Do references to play or work rhetoric in Victorian Scientific Life Writing increase over the course of its run? No, although fascinatingly references to work do seem to go down slightly over time.

```{r HouseholdWordFreqmat Visual,  eval=FALSE}
# Visualizing HouseholdWordsFreqdf BY DATE
      p <- ggplot(HouseholdWordsFreqdf, aes(y = as.numeric(as.character(Normalized_Freq)), x = as.numeric(substr(Date,1,4)), color = Category, label = Sample_KWIC))
      pg <- geom_point(size=1,pch = 16)
      pl <- p + pg + labs(x = "Date", y = "Normalized Frequency (% of Words in Text)", title = "Appearances of Play and Work Rhetoric within Household Words")
      ggplotly(pl)
```

We can also visualize the terms which most frequently occur around the search terms in the two categories within this corpus. Both contain a lot of masculine pronouns, but around play we get theater, music, and childhood (as expected), while around work we get a lot of numbers.
```{r Household Words Work/Play association,  eval=FALSE}
HouseholdWordsWordFlagdf$KWIC <- as.character(HouseholdWordsWordFlagdf$KWIC)
HouseholdWordsWordFlagdf$Text <- as.character(HouseholdWordsWordFlagdf$Text)
corpus <- corpus(HouseholdWordsWordFlagdf, 
                 docid_field="Text", 
                 text_field="KWIC")
group_HouseholdWordsWordFlagdfm <- dfm(corpus, remove=c(stopwords("en"),HouseholdWordssearchedtermlist), remove_punct=TRUE, remove_numbers = TRUE, groups="Category")
textplot_wordcloud(group_HouseholdWordsWordFlagdfm,max.words=50, colors = RColorBrewer::brewer.pal(8,"Dark2"), comparison=TRUE)
```
